\section{Introduction}
PhEDEx \cite{PhEDEx} is the data-placement management tool for the CMS \cite{CMS} experiment at 
the LHC. It manages the scheduling of all large-scale WAN transfers in CMS, 
ensuring reliable delivery of the data. It consists of several components:

\begin{itemize}
\item an Oracle database, hosted at CERN
\item a website and data-service, which users (humans or machine) use to interact with and control 
PhEDEx
\item a set of 'central' agents that deal with routing, request-management, bookeeping and other 
activities. These agents are also hosted at CERN, though they could be run anywhere. The 
key point is that there is only one set of central agents per PhEDEx instance
\item a set of 'site-agents', one set for every site that receives data
\end{itemize}

PhEDEx maintains knowledge and history of transfer 
performance, and the central agents use that information to choose among source replicas when a 
user makes a request. The central agents then queue the transfer to be processed by the site 
agents. PhEDEx operates in a data-pull mode, the destination site pulls the data to itself when it 
is ready. This gives the sites more control over the activity at their site, so they can ensure 
that neither their network nor their storage is overloaded.

PhEDEx was originaly conceived over ten years ago now, and the architecture still reflects design 
decisions made at that time. Then, the network was expected to be the weakest link in 
the developing Worldwide LHC Grid (WLCG) \cite{WLCG}. Networks were expected to have bandwidth of 
the order of 100 Mb/sec, to be unreliable, and to be poorly connected across the span of the CMS 
experiment. Accordingly, PhEDEx will back off fast and retry gently in the face of failed 
transfers, on the assumption that failures will take time to fix, and that there is other useful 
data that can use the network in the meantime.

The data-transfer topology was designed with a 
strongly hierarchical structure, with the Tier-0 (CERN) transferring data primarily to a set of 
6-7 Tier-1 sites, and each Tier-1 site handling traffic between itself, the other Tier-1s. and 
it's local Tier-2 sites. This kept the transfer-links (i.e. the set of (source,destination) pairs) 
mostly in the realm of a single regional network operator, and kept the overall number of transfer 
links low. Transfers between arbitrary pairs of sites were not allowed.

Today, the reality is very different. The network has emerged as the most reliable component.
